{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Vectorization NLP",
      "provenance": [],
      "authorship_tag": "ABX9TyMxUjsRCpFWIbSoA3XLgP6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishav-hub/NLP-resources/blob/main/Text_Vectorization_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Text Data with scikit-learn"
      ],
      "metadata": {
        "id": "1pQs9i7odlvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Counts with CountVectorizer\n",
        "\n",
        "The CountVectorizer provides a simple way to both tokenize a collection of text documents\n",
        "and build a vocabulary of known words, but also to encode new documents using that vocabulary"
      ],
      "metadata": {
        "id": "YuGU2xYhzwSX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yxd7TByjcXhB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4498484a-2a7e-4f63-b2c8-26ad1debe7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "(1, 8)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode another document\n",
        "text2 = [\"the puppy\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS7nTujlzzAQ",
        "outputId": "3182c9aa-4281-4c52-b27a-2eaaadbafd68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Word Frequencies with TfidfVectorizer\n",
        "\n",
        "\n",
        "Word counts are a good starting point, but are very basic. One issue with simple counts is that\n",
        "some words like the will appear many times and their large counts will not be very meaningful\n",
        "in the encoded vectors. An alternative is to calculate word frequencies, and by far the most\n",
        "popular method is called TF-IDF. This is an acronym that stands for Term Frequency - Inverse\n",
        "Document Frequency which are the components of the resulting scores assigned to each word.\n",
        "- Term Frequency: This summarizes how often a given word appears within a document.\n",
        "- Inverse Document Frequency: This downscales words that appear a lot across documents.\n"
      ],
      "metadata": {
        "id": "UZkqcSyIz9EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "\"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJXlKTMlz1pF",
        "outputId": "621f7aa0-4506-40ff-da26-cc0037c59ce9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "(1, 8)\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Prepare Text Data With Keras"
      ],
      "metadata": {
        "id": "ahWZmamM6_so"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Split Words with text to word sequence\n",
        "\n",
        "A good first step when working with text is to split it into words. Words are called tokens and the process of splitting text into tokens is called tokenization. Keras provides the\n",
        "text to word sequence() function that you can use to split text into a list of words."
      ],
      "metadata": {
        "id": "UDAZoABn7Kvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sD5_I-U30Xa4",
        "outputId": "ee97c7e0-5443-4468-acec-4efbba5064f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding with one hot\n",
        "\n",
        "It needs the vocab size which is the maximum size of the tokens or can be more if we intend to add more documents. It will split at white spaces and lower case the tokens."
      ],
      "metadata": {
        "id": "BF89X-Jc715j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYISjpW47S0Y",
        "outputId": "ce809f7f-9cd9-477e-c880-8b12bfdb2a93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[2, 7, 7, 5, 6, 3, 2, 6, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hash Encoding with hashing trick\n",
        "\n",
        "- A limitation of integer and count base encodings is that they must maintain a vocabulary of\n",
        "words and their mapping to integers. An alternative to this approach is to use a one-way hash\n",
        "function to convert words to integers. This avoids the need to keep track of a vocabulary, which\n",
        "is faster and requires less memory\n",
        "\n",
        "- It provides more flexibility, allowing you to specify\n",
        "the hash function as either hash (the default) or other hash functions such as the built in md5\n",
        "function or your own function."
      ],
      "metadata": {
        "id": "hrrHV5E08y7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import hashing_trick\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "\n",
        "\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHTJ3V-28P7f",
        "outputId": "990cb696-74b6-4996-bed3-3f712f49a10d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer API\n",
        "\n"
      ],
      "metadata": {
        "id": "INNy0_k69qUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n"
      ],
      "metadata": {
        "id": "8gE2ytZS9J2f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize what was learned / 4 attributes\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcI9AyG296vO",
        "outputId": "cdf84c20-f806-4c96-90a2-91194a855b42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# integer encode documents\n",
        "\n",
        "encoded_docs_count = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs_count)\n",
        "encoded_docs_tfidf = t.texts_to_matrix(docs, mode='tfidf')\n",
        "print(\"Using TFIDF\")\n",
        "print(encoded_docs_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX4rqVu09-1N",
        "outputId": "7242a7d8-ffbf-4339-b506-d33a27da7db8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Using TFIDF\n",
            "[[0.         0.         1.25276297 1.25276297 0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.98082925 0.         0.         1.25276297 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         1.25276297\n",
            "  1.25276297 0.         0.        ]\n",
            " [0.         0.98082925 0.         0.         0.         0.\n",
            "  0.         1.25276297 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.25276297]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HjCRlbLz-NsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}